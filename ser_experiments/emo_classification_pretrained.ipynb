{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gzuv2run9Yxa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    ASTForAudioClassification\n",
    ")\n",
    "\n",
    "from typing import List, Optional, Union\n",
    "import librosa\n",
    "from transformers import SequenceFeatureExtractor, BatchFeature, TensorType\n",
    "\n",
    "from utils import compute_score, train\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED) \n",
    "    torch.manual_seed(SEED) \n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED) \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5MpBxb2MjsO"
   },
   "source": [
    "## Просмотр данных и формирование датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В обучающей выборке 9321 аудиозаписей\n",
      "В валидационной выборке 815 аудиозаписей\n",
      "В тестовой выборке 833 аудиозаписей\n"
     ]
    }
   ],
   "source": [
    "DIR_TRAIN = \"wavs/train_\"\n",
    "DIR_VAL = \"wavs/val_\"\n",
    "DIR_TEST = \"wavs/test_\"\n",
    "\n",
    "PATH_TRAIN = \"train.csv\"\n",
    "PATH_VAL = \"val.csv\"\n",
    "PATH_TEST = \"test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(PATH_TRAIN)\n",
    "df_val = pd.read_csv(PATH_VAL)\n",
    "df_test = pd.read_csv(PATH_TEST)\n",
    "\n",
    "print(\"В обучающей выборке {} аудиозаписей\".format(len(df_train)))\n",
    "print(\"В валидационной выборке {} аудиозаписей\".format(len(df_val)))\n",
    "print(\"В тестовой выборке {} аудиозаписей\".format(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h72XzlyfMZek"
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, train_df, val_df, test_df):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            train_df, val_df, test_df (pd.DataFrame): тренировочный, валидационный и тестовый наборы данных\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train_df = train_df\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = val_df\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train') \n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset(cls, path_train, path_val, path_test, features_dir=None, \n",
    "                     dir_train=None, dir_val=None, dir_test=None,\n",
    "                     feature_extractor=None, load_features=True, save_features=False):\n",
    "        \"\"\"Загрузка данных датасета\n",
    "        Аргументы:\n",
    "            path_train, path_val, path_test (str): пути к тренировочному, валидационному и тестовому наборам данных\n",
    "            features_dir (str): директория признаков аудиозаписей\n",
    "            dir_train, dir_val, dir_test (str): директории с аудиозаписями\n",
    "            feature_extractor (): объект класса, извлекающего признаки из аудиозаписи\n",
    "            load_features (bool): True -> загрузка выделенных ранее признаков, \n",
    "                                  False -> выделение признаков из аудиозаписей\n",
    "            save_featurees (bool): True -> сохранение выделенных признаков в features_dir\n",
    "        Возвращает:\n",
    "            экземпляр EmotionDataset\n",
    "        \"\"\"\n",
    "        #загрузка данных\n",
    "        train_df = pd.read_csv(path_train)\n",
    "        val_df = pd.read_csv(path_val)\n",
    "        test_df = pd.read_csv(path_test)\n",
    "        \n",
    "        #загрузка признаков\n",
    "        if load_features:\n",
    "            features = cls._load_features(train_df, 'train', features_dir)\n",
    "            train_df.insert(len(train_df.columns), 'features', features)\n",
    "\n",
    "            features = cls._load_features(val_df, 'val', features_dir)\n",
    "            val_df.insert(len(val_df.columns), 'features', features)\n",
    "\n",
    "            features = cls._load_features(test_df, 'test', features_dir)\n",
    "            test_df.insert(len(test_df.columns), 'features', features)\n",
    "        \n",
    "        #векторизация аудиозаписей и сохранение признаков\n",
    "        else:\n",
    "            \n",
    "            features = cls._extract_features(train_df, dir_train, feature_extractor,\n",
    "                                             'train', features_dir, save_features)\n",
    "            train_df.insert(len(train_df.columns), 'features', features)\n",
    "\n",
    "            features = cls._extract_features(val_df, dir_val, feature_extractor, \n",
    "                                             'val', features_dir, save_features)\n",
    "            val_df.insert(len(val_df.columns), 'features', features)\n",
    "            \n",
    "            features = cls._extract_features(test_df, dir_test, feature_extractor, \n",
    "                                             'test', features_dir, save_features)\n",
    "            test_df.insert(len(test_df.columns), 'features', features)\n",
    "        \n",
    "        return cls(train_df, val_df, test_df)\n",
    "    \n",
    "    def _extract_features(df, files_dir, feature_extractor, mode, features_dir=None, save_features=False):\n",
    "        \"\"\"Выделение признаков\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        pbar = tqdm.tqdm(range(len(df['hash_id'])), total=len(df['hash_id']))\n",
    "        pbar.set_description(mode + ' dataset loading')\n",
    "        for i in pbar:\n",
    "            waveform, sampling_rate = torchaudio.load(files_dir + df['audio_path'][i])\n",
    "            waveform = waveform.squeeze().numpy()\n",
    "            feature = feature_extractor(waveform, sampling_rate).input_values[0]\n",
    "            features.append(feature)\n",
    "            if save_features:\n",
    "                np.save('{0}{1}/{2}.npy'.format(features_dir, mode, df['hash_id'][i]), feature)\n",
    "        return features\n",
    "    \n",
    "    def _load_features(df, mode, features_dir):\n",
    "        \"\"\"Загрузка ранее выделенных признаков\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        pbar = tqdm.tqdm(range(len(df['hash_id'])), total=len(df['hash_id']))\n",
    "        pbar.set_description(mode + ' dataset loading')\n",
    "        for i in pbar:\n",
    "            feature = np.load('{0}{1}/{2}.npy'.format(features_dir, mode, df['hash_id'][i]))\n",
    "            features.append(feature)\n",
    "        return features\n",
    "        \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"Выбор фрагментов набора данных по столбцу из объекта dataframe\n",
    "        Аргументы:\n",
    "            split (str): \"train\"/\"val\"/\"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[self._target_split]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self._target_df.iloc[idx]\n",
    "        return {\"features\": row['features'],\n",
    "                \"class\": row['emotion']}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дообучение Audio Spectrogram Transformer (AST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "CLASSES = ['positive', 'sad', 'angry', 'neutral']\n",
    "DIR_FEATURES = 'ast_features/'\n",
    "EXPERIMENT_DIR = 'ast_experiments/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_PATH)\n",
    "feature_extractor = lambda waveform, sampling_rate: auto_feature_extractor(waveform, sampling_rate=sampling_rate, \n",
    "                                                                           return_tensors=\"np\")\n",
    "ds = EmotionDataset.load_dataset(PATH_TRAIN, PATH_VAL, PATH_TEST, features_dir=DIR_FEATURES,\n",
    "                                 #dir_train=DIR_TRAIN, dir_val=DIR_VAL, dir_test=DIR_TEST, \n",
    "                                 #feature_extractor = feature_extractor, load_features=False, save_features=True)\n",
    "                                 feature_extractor = feature_extractor, load_features=True)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Дообучение всей сети (трансформер + классификатор)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Одинаковое изменение lr для всех параметров сети в процессе обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam\n",
    "- batch_size: 8\n",
    "- scheduler: после 2 эпохи lr уменьшается в 2 раза каждую эпоху\n",
    "- base_lr: 1e-5, 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 2\n",
    "SEED = 42\n",
    "EXP_NUM = 1 #1\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)#, momentum=0.9)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=4)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 2\n",
    "SEED = 42\n",
    "EXP_NUM = 2 #13\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=4)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Разное изменение lr в процессе обучения для всех параметров трансформера и параметров классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam\n",
    "- batch_size: 12\n",
    "- scheduler: \n",
    "    - для трансформера: после 2 эпохи lr уменьшается в 2 раза каждую эпоху\n",
    "    - для классификатора: постоянный lr\n",
    "- base_lr: 1e-5, 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 3\n",
    "SEED = 42\n",
    "EXP_NUM = 3 #14\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = [optim.Adam(model.audio_spectrogram_transformer.parameters(), lr=1e-5),\n",
    "             optim.AdamW(model.classifier.parameters(), lr=1e-5)\n",
    "            ]\n",
    "sсheduler = [optim.lr_scheduler.MultiStepLR(optimizer[0], milestones=np.arange(2, EPOCH), gamma=0.5),\n",
    "             optim.lr_scheduler.MultiStepLR(optimizer[1], milestones=[EPOCH + 1])\n",
    "            ]\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=4)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 3\n",
    "SEED = 42\n",
    "EXP_NUM = 4 #15\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = [optim.Adam(model.audio_spectrogram_transformer.parameters(), lr=1e-6),\n",
    "             optim.AdamW(model.classifier.parameters(), lr=1e-6)\n",
    "            ]\n",
    "sсheduler = [optim.lr_scheduler.MultiStepLR(optimizer[0], milestones=np.arange(2, EPOCH), gamma=0.5),\n",
    "             optim.lr_scheduler.MultiStepLR(optimizer[1], milestones=[EPOCH + 1])\n",
    "            ]\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=4)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Обучение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Уменьшение lr в 2 раза каждую эпоху после 2 эпохи обучения (Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam (оптимизация параметров классификатора)\n",
    "- batch_size: 12\n",
    "- scheduler: после 2 эпохи lr уменьшается в 2 раза каждую эпоху\n",
    "- base_lr: 1e-5, 1e-2, 5e-2, 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 5 #2\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-5)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 6 #3\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 7 #4\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=5e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 8 #5\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-1)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Уменьшение lr в 2 раза каждую эпоху после 2 эпохи обучения (AdamW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: AdamW (оптимизация параметров классификатора)\n",
    "- batch_size: 12\n",
    "- scheduler: после 2 эпохи lr уменьшается в 2 раза каждую эпоху\n",
    "- base_lr: 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 9 #6\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Постоянный lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam, AdamW (оптимизация параметров классификатора)\n",
    "- batch_size: 12\n",
    "- scheduler: постоянный lr\n",
    "- base_lr: 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 10 #7\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[EPOCH + 1], gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 11 #8\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[EPOCH + 1], gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Уменьшение lr в 2 раза каждую 2-ую эпоху после 2 эпохи обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam (оптимизация параметров классификатора)\n",
    "- batch_size: 12\n",
    "- scheduler: после 2 эпохи lr уменьшается в 2 раза каждую 2-ую эпоху\n",
    "- base_lr: 1e-2, 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 12 #9\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH, 2), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 13 #10\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-1)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH, 2), gamma=0.5)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Уменьшение lr на четверть каждую эпоху после 2 эпохи обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры эксперимента:\n",
    "- optimizer: Adam (оптимизация параметров классификатора)\n",
    "- batch_size: 12\n",
    "- scheduler: после 2 эпохи lr уменьшается на 1/4 каждую эпоху\n",
    "- base_lr: 1e-2, 5e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 11\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=1e-2)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.75)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 12\n",
    "SEED = 42\n",
    "EXP_NUM = 12\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "\n",
    "for param in model.audio_spectrogram_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=5e-1)\n",
    "sсheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=np.arange(2, EPOCH), gamma=0.75)\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=1)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5\n",
    "BATCH_SIZE = 2\n",
    "SEED = 42\n",
    "EXP_NUM = 16\n",
    "\n",
    "set_seed(SEED)\n",
    "    \n",
    "model = ASTForAudioClassification.from_pretrained(MODEL_PATH, num_labels=len(CLASSES), \n",
    "                                                  return_dict=False, ignore_mismatched_sizes=True)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = [optim.Adam(model.audio_spectrogram_transformer.parameters(), lr=1e-6),\n",
    "             optim.AdamW(model.classifier.parameters(), lr=1e-6)\n",
    "            ]\n",
    "sсheduler = [optim.lr_scheduler.MultiStepLR(optimizer[0], milestones=np.arange(2, EPOCH), gamma=0.5),\n",
    "             optim.lr_scheduler.MultiStepLR(optimizer[1], milestones=[EPOCH + 1])\n",
    "            ]\n",
    "\n",
    "writer =  SummaryWriter(\"{0}runs/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM))\n",
    "writer.add_graph(model, input_to_model=torch.Tensor(np.array([ds[i]['features'] \n",
    "                                                              for i in range(BATCH_SIZE)])).to(device, torch.float32))\n",
    "\n",
    "train(model, ds, loss_func, optimizer, sсheduler, EPOCH, BATCH_SIZE, device, writer, CLASSES,\n",
    "      \"{0}checkpoints/exp{1}\".format(EXPERIMENT_DIR, EXP_NUM), gradient_accumulation_steps=4)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chatbot_gpu",
   "language": "python",
   "name": "chatbot_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
